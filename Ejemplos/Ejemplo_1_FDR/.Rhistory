rm(list=ls())
setwd("C:/R")
rm(list=ls())
setwd("C:/R")
data <- read.csv("data/ratSurvival.csv")
str(data)
summary(data)
levels(data$regimen)
data.control <- subset(data, regimen=="control" & gender=="female") # Extract the control group of females
data.control$time
# Only 2 rats of this group died before the end of the experiment. On the other hand, 7 females of the group fed with 22% of maize NK693 died during the experiment.
data.test <- subset(data, regimen=="NK603-22%" & gender=="female")
data.test$time
# Because of the (right) censoring process, we cannot just compare the mean survival times using a t-test. On the other hand, we can use the Wilcoxon-Mann-Whitney test which precisely aims to compare the ranks of the survival times in both groups.
wilcox.test(data.test$time, data.control$time, alternative="less") #  tie가 있어 정확한 p값을 계산할 수 없습니다 = 절대값 때문
library(ggplot2) ; theme_set(theme_bw())
pval <- stat <- gender <- regimen <- NULL # Zero vectors
for (g in levels(data$gender)) {
data.control <- subset(data, regimen=="control" & gender==g)
for (r in levels(data$regimen)) {
if (r != "control") {
data.test <- subset(data, gender==g & regimen==r)
wt <- wilcox.test(data.test$time, data.control$time, alternative = "less")
pval <- c(pval, wt$p.value)
stat <- c(stat, wt$statistic)
gender <- c(gender, g)
regimen <- c(regimen, r)
}
}
}
R <- data.frame(gender=gender, regimen=regimen, stat=stat, p.value=pval)
R
R <- R[order(R$p.value),]
R
# Plotting...
ggplot(data=R) + geom_point(aes(x=1:18, color=regimen, y=p.value, shape=gender), size=4) +
scale_y_log10() + xlab("p-value") + scale_x_continuous(breaks=NULL)
data <- read.csv("data/dietary.csv")
data
p.bh <- data$p.value
m <- length(p.bh)
for (i in ((m-1):1))
p.bh[i] <- min(data$p.value[i]*m/i , p.bh[i+1])
data$p.bh <- p.bh
alpha <- 0.25
m <- dim(data)[1]
data$critical.value <- (1:m)/m*alpha
data
library(gridExtra)
pl1 <- ggplot(data) + geom_line(aes(x=1:m,y=p.value), colour="blue") +
geom_line(aes(x=1:m,y=critical.value), colour="red") +xlab("(i)")
grid.arrange(pl1,pl1 + xlim(c(1,8)) + ylim(c(0,0.21)) + geom_vline(xintercept=5.5))
m0.est <- sum(data$p.bh>alpha)
data$crit.valc <- round(data$critical.value*m/m0.est,4)
data$p.bhc <- round(data$p.bh*m0.est/m,4)
head(data,10)
nx <- 50
ny <- 50
m0 <- 120
m1 <- 20
mu.x <- 0
mu.y <- c(rep(0, m0), seq(0.3, 0.6, length=m1))
# For each of the L=1000 simulated replicate of the same experiment, we will randomly sample observation x and y from the model and perform a t-test for each of the m=140 variables. We therefore get m=140 p-values for each of these L=1000 replicates.
L <- 1000
m <- m0+m1
set.seed(1234)
pval <- matrix(ncol=L, nrow=m)
for (l in (1:L)) {
x.sim <- matrix(rnorm(nx*m, mu.x), ncol=nx)
y.sim <- matrix(rnorm(ny*m, mu.y), ncol=ny)
dat <- cbind(x.sim, y.sim)
pval[,l] <- apply(dat, 1, function(dat) {
t.test(x = dat[1:nx], y = dat[(nx +  1):(nx + ny)])$p.value})
}
alpha=0.2
m01 <- colSums(pval[1:m0,] < alpha)
mean(m01/m0) # the proportion of wrongly rejected null hypotheses
m11 <- colSums(pval[(m0+1):m,] < alpha)
mean(m11/m1) # The proportion of correctly rejected null hypotheses is an estimate of the power of the test
mean(m01/(m01+m11)) #  about 60% of them are false discoveries.
# Let us now apply the Bonferroni correction, and compute the proportion of wrongly and correctly rejected null hypotheses and the proportion of false discoveries
pval.b <- apply(pval,2, function(pval) {p.adjust(pval, method="bonferroni")})
m01.b <- colSums(pval.b[1:m0,] < alpha)
m11.b <- colSums(pval.b[(m0+1):m,] < alpha)
c(mean(m01.b/m0), mean(m11.b/m1), mean(m01.b/(m01.b+m11.b), na.rm=TRUE))
# Very few of the true null hypotheses are rejected, which may be a good point, but the price to pay is a very low power (less that 20%).
# The familywise error rate (FWER) is the probability P(m01≥1) to reject at least one of the true null hypothesis. This probability remains quite small when the Bonferroni correction is used.
mean(m01.b>=1)
# The Benjamini-Hochberg correction increases the power and controls the FDR as expected since the proportion of false discoveries remains below the level α.
pval.bh <- apply(pval,2, function(pval) {p.adjust(pval,method="BH")})
m01.bh <- colSums(pval.bh[1:m0,] < alpha)
m11.bh <- colSums(pval.bh[(m0+1):m,] < alpha)
c(mean(m01.bh/m0), mean(m11.bh/m1),mean(m01.bh/(m01.bh+m11.bh),na.rm=TRUE))
# Lastly, we can slightly improve the BH procedure by multiplying the p-values by the ratio m^0⋅/m
pval.bhc <- pval.bh
for (l in (1:L)) {
m0e <- sum(pval.bh[,l]>alpha)
pval.bhc[,l] <- pval.bh[,l]*m0e/m
}
m01.bhc <- colSums(pval.bhc[1:m0,] < alpha)
m11.bhc <- colSums(pval.bhc[(m0+1):m,] < alpha)
c(mean(m01.bhc/m0), mean(m11.bhc/m1),mean(m01.bhc/(m01.bhc+m11.bhc),na.rm=TRUE))
